---
title: "Final Draft"
author: "Layla Quinones, Jeff Shamp, Sam Bellows, Neil Shah, & Angel Claudio"
date: "3/21/2020"
output: rmdformats::material
number_sections: yes
params: 
  pwd: "data607"
---

# Introduction

While the analysis of data is nothing new, the growth of data sources, programming languages and visual platforms has catapulted the  term and profession “data scientist” into the everyday life. The overall goal of CUNY’s Data Science program is to equip aspiring data scientists or sharpen seasoned professionals with the knowledge and skills to succeed in this new data-driven ecosystem. However, like any evolving profession, there isn’t a concrete template of what makes a data scientist, and whether the tools and skillsets are static over time. 
With that basis, our team seeks to define and then answer **“What are the most valued data science skills?”**.

```{r, echo=FALSE, out.width = '50%', fig.align = "center"}
knitr::include_graphics("https://github.com/MsQCompSci/images/blob/master/introPng.png?raw=true")
```

# Planning, Collaboration and Team Structure  {.tabset}

We began with a brainstorming session where each team member delivered data sources that they thought would be useful for the project. We selected sources after evaluating datasets based on robustness, relevance and overall functionality, and evaluating proof of concepts created by team members. The sources we ultimately decided on was the Kaggle 2019 Survey data at https://www.kaggle.com/c/kaggle-survey-2019/data, KD Nuggets on Core Data Science Skills from https://www.kdnuggets.com/2019/09/core-hot-data-science-skills.html, and finally Google Trends Web API to explore data science tool popularity at https://trends.google.com/trends/.


We continued to hold voice meetings approximately twice a week in our discord server to update each other on our progress and make sure we maintained a unified goal for the project. For our main repository we used https://github.com/Shampjeff/data_607_project3, where we forked and pushed back our individual work to the dev branch to be merged with the master branch.

```{r, echo=FALSE, out.width = '50%', fig.align = "center"}
knitr::include_graphics("https://github.com/MsQCompSci/images/blob/master/gitCollab.png?raw=true")
```

## Angel

The data from Kaggle was of concerning size, messy, and needed to be normalized. We even noticed that when dealing with the data we had memory issues because of all the object references and graph rendering would cause R Studio execution to fail. The agreed upon solution was to set up an Azure Cloud SQL Server Database. 

In order to make working with the database easy, I configured the firewall to be open for all IP addresses - we had to be extra careful with handling the password since Azure Cloud Databases charge based on transactional processing, and well a hacker could potentially create a financial nightmare. Therefore since the firewall was open, as long as you had the SQL Server Authentication credentials and the URL for the cloud instance, team members were good to go for leveraging the backend with full admin rights to meet their needs.

In summary the data from R was carefully prepped  to import with 3 particular divisions in mind, creating the main table for participants in the survey, creating the reference tables for the underlying values of the main table for optimization, and creating the artifact tables from the original wide data set that were of very specific interest for analysis. Indexing was avoided since the entirety of the entities were being used and we wanted to demonstrate in R, rather than in T-SQL, how to manipulate the sets for analysis. However, a View object was created for the normalized data and a Schema object as well to seperate the Kaggle part of the project making the database scaleable for other sources or modules of the project.

## Sam 

I started by doing EDA on the Kaggle data cleaned by Jeff. As I worked, I discovered further cleaning that was necessary like adjusting country names or education categories. I then created a function to automatically visualize the large amounts of data and different skills that were present in the dataset. I made sure that the graphics were presentation level and drew conclusions from said visualizations. Once Angel had the database running, I integrated the database into my code by pulling my data straight from the database.

Other Duties:

- Final Project edits

## Jeff

I wanted to find a dataset that could quantify skills and demographics for people in the data science field and the natural choice for me was the Kaggle Data Science Survey 2019. This is the most recent and arguably most complete set of data regarding data science professionals. More about the survey can be found [here](https://www.kaggle.com/c/kaggle-survey-2019). I used only the multiple choice answers as it seemed well structured and rich with information. I cleaned and formatted the data for ease of use and for upload to our cloud SQL server. 

Next, I wanted to explore the question how to best fill out or build a data science team as that would naturally lead us to the most valuable skills and technologies in the field. I took the Kaggle survey results and performed a K Means clustering analysis to find the optimum number of groups that exemplify the types of skills a good data science team should have. 

Other Duties:

- Github administrator and merge conflict resolver

## Layla

During my initial research I set out to answer: 

**What data science skills and tools are currently used in the profession?** 

I began by first scraping the web for any information we can find about skills data scientists want/need to do their jobs. Amoung the many sources found, the following helped us identify key skills, platforms and programming languages that are gaining popularity within the Data Science community. You can access the rmd file with ALL the research found at this stage [HERE](https://raw.githubusercontent.com/Shampjeff/data_607_project3/master/scrapingPractice.Rmd)

I initially tried to gather information from tables saved on webpages by creating a scraper that would search tables, and one that searched for key words on a collection of web pages and tallied the amount of time those key words appear on a given web page. In the end, although some really cool scraping was done, as a group we decided to retain only 3 graphs that identify key terms for Data Science Skills, Platforms and Programming Languages, and use an alternative method to talley keywords. The key terms were then passed onto Neil for him to use in analysing data from google searches.


Other Duties:

- Template for final group project

- Final Project edits, images etc

## Neil

My task was to investigate and quantify how different keywords related to Data Science skills changed over time in terms of popularity, and against each other. To do I chose to utilize Google Trends data as a proxy for popularity and cross-validae other data-sets [specifically Layla's KD Nuggets data keywords].

I developed an automated work flow to sanitize key words, generate time-series hit data from the gTrendsR package, clean the data into a dataframe, perform cursory statistical analysis, rapidly plot the trends and finally quantify point in time changes. 

Overall my body of work encompassed a creation of a new data-set, comparison to an existing one and laying the framework for furhter rapid analysis.

Other dutes
- Content editor and group's unofficial "Motivator in Chief", promising to award sucessful milestone completions with pints

# Results and Findings{.tabset}

## Kaggle Part I Analysis

Based on the Kaggle data set, we can start to draw conclusions about what skills are highly valued by employers in the data science market. Please note that these conclusions are drawn based on self-reported salaries and would need to be verified with better data.

Many of the findings are not actual skills per se: the number one predictor of salary turned out to be whether a person lived in India or the United States. Similar "skills" include whether or not the person is male or the age of said person. (I do not espouse these views as correct, I merely show what the data say.)

In terms of ACTUAL skills valued by employers, some things that jump out from the visualizations.

- Having a masters/doctoral degree
- Many years of coding experience
- Cloud based data software such as AWS/Azure
- Advanced statistical software such as SPSS or SAS
- R/Bash have the highest salary, but Python most popular
- Seaborn and Altair
- Neural Networks and Gradient Boosting
- AWS is both popular and highly valued
- SQL is a valuable skill

## Kaggle Part II Analysis

What types of skills make up a good data scientist and good data science teams?

Our clusters give us nice insight into this so let's summarize the cluster types as best we can. Cluster two are novices and will not be covered. 

* **Cluster one:** These are people with a lot experience in the field. They can or have used many types of software including the popular languages and traditional software engineering languages. They are early users of cloud technologies and big data systems. These folks can be older or are managers of teams or divisions at this point. 

* **Cluster three:** These are folks who work in a traditional sense of data science and analysis. They use the standard canon of the technologies; R, Python, local SQL platforms, and text editors. They like R and seem to not like Jupyter and do not have much experience with cloud or big data systems. 

* **Cluster four:** These folks are similar to cluster three, but with a few key differences. They like to use RStudio, but not the R language. They like Jupyter and are ramping up their skills with cloud products and big data systems. These people seem to be mid-career and in transition as they are younger than cluster one but a bit older than cluster three. 

### Findings

Cloud products and big data systems are highly in-demand as the higher paid people in clusters one and four both have skill sets in these areas. Those clusters also have deeper experience in the field on average. They are also well versed in the traditional technologies and languages for data analysis. I would want them on my team. I would also want a few people, perhaps working under a cluster one or four person, to work with smaller datasets for manipulation, tidying, visualizing, and statistical work. 

## Web Scraping & Google Trend Analysis 

### Top Trends in Software

1. Python exhibited the same trend in both data sets--it was the most overall popular and gained popularity.

2. Both data sets reflected decreasing popularity within Excel, SQL and R. 

3. Both data sets reflected a sharp bump in 2018 and in many cases peak interest/hit.

4. Both data sets reflected decreasing popularity of Apache Spark--but Gtrends showed a sharp decline.

5. Both data sets reflected increasing popualarity in Anaconda, TensorFlow, Keras, and Scikit--though the magnitudes were more in Gtrends.

6. A key difference is RapidMinr--while KDnuggets had it growing in popularity, Gtrends did not reflect.

**Overall Gtrends and KdNuggets had a similar trend and direction among the key Data Science Tools used**

### Deep Learning Platforms

1. Only Pytorch and TFlearn had the same directions in both data sets--specifically Pytorch having the largest positive magnitude increase. TFlearn popularity decreased in both data sets; however, in Gtrends it had the largest magnitude percent change.
 
2. Interestingly enough all other Deep Learning Platform trends had the opposite trends between KDNuggets and Google Trends--some like Theano (negative in KDnuggets and positive in Gtrends) were significantly different. 

3. Apache, Microsoft Cognitive Toolkit and Deep Learning tools registered no change in Gtrends but specifically they had no hits.

4. Looking at Keras and Torch had the opposite directions between our data sets but by a small magnitude--keeping in mind our data comparsion was just two points, this could of been just due to sampling fluctuations. 

Overall the main conclusion that we can draw is that **Pytorch and TFlearn were consistent between both datasets, that Pytorch has gained recent popularity while TFlearn has not**.

### Programming Language Trends

1. Julia was the most positive in percent change on both data sets, however the Google Trends magnitude was much lower.

2. In fact other than Python and Other Programming Languges--all Programming platforms had negative percent change in Google Trends. C/C++ had the most decrease in Google Trends despite it being slightly positive in KD Nuggets

3. Lisp and Perl had opposite trends between KDNuggets (being slightly positive change) versus negative in Google Trends.

Looking at the correlated data we can conclude

1. Julia is indeed the popular emergy programming platform, being positive in both datasets.

2. Unix, Java, SQL and Scala have lost popularity over time

## Conclusions

Combining the findings from all our individual data exploration approaches we have found the following answers to our question:

1. One of the most valued data science skills in unsurprisingly coding experience and machine learning experience.

2. Python appears to be more popular than R, although R had higher median salaries.

3. It appear that there is some gap between Python and R: Users often prefer one to the other and often don't have experience with both.

4. Julia has gained the most popularity recently, but is still not as popular as R or Python.

5. Cloud products are very popular, becoming highly in demand and correlating with high salaries.

6. Machine learning platforms and skills appear to still be popular today, although not necessarily correlated with high salaries.

7. On the whole, interest in data science skills appeared to peak in 2018, and is not growing at the same rate today.

8. There appears to be decreasing interest in Excel, SQL, and R; However, SQL and R are still correlated with higher salaries.

# Methodology, Analysis & Limitations {.tabset}

## Library Setup

```{r setup, message=F, warning = FALSE}
#TOOLS FOR DB CONNECTIONS AND ANALYSIS OF KAGGLE DATA
library(odbc)
library(DBI)
library(ggmosaic)
library(RMySQL)
library(fastDummies)
library(DBI)
library(magrittr)

#TOOLS FOR WEB SCRAPING AND ANALYSIS
library(rvest)
library(stringi)
library(tidyverse)
library(RCurl)
library(RColorBrewer)
library(xml2)
library(kableExtra)
library(Stack)
library(gtrendsR)
library(ggplot2)
library(reshape2)
library(stringr)
library(cowplot)
```



## Kaggle Data Munging {.tabset}


### Import the Data to R

Below is the bulk data file from Kaggle:

```{r eval = F}
file <- "./Data/multiple_choice_responses.csv"

df <- read.csv(file = file, sep = ",", stringsAsFactors = F, na.strings = T) %>% 
              mutate_if(is.character, list(~na_if(.,""))) 
```

### Data Wrangling in R

The first table we want is the basic information; age, country, education level, company size, salary, experience level, etc. 

```{r eval = F}
df_basic<- df %>%
  select(Q1:Q2,Q3:Q4,Q6:Q8, Q10,Q11,Q15) %>%
  rename(age = Q1,         # custom name for columns
         gender = Q2, 
         country = Q3,
         education = Q4,
         company_size = Q6,
         ds_team_size = Q7,
         company_use_ml = Q8,
         compensation_USD = Q10,
         spend_ml_cloud_work_USD = Q11,
         code_exp_years = Q15) %>%
  slice(2:(dim(df)[1])) %>%
  mutate(                  # Following mutatations to clean formatting
    country = str_replace_all(country,
                "United Kingdom of Great Britain and Northern Ireland",
                "United Kingdom")) %>%
  mutate(company_size = str_remove_all(company_size, "employees")) %>%
  mutate(education = str_remove_all(education, "degree")) %>%
  mutate(education = str_remove_all(education, "[:punct:]")) %>%
  mutate(compensation_USD = str_remove_all(compensation_USD, "[$]")) %>%
  mutate(spend_ml_cloud_work_USD = str_remove_all(spend_ml_cloud_work_USD,"[$]")) %>%
  mutate(spend_ml_cloud_work_USD = str_remove_all(spend_ml_cloud_work_USD,"\\(USD\\)")) %>%
  mutate(code_exp_years= str_remove_all(code_exp_years, "years")) %>%
  mutate(code_exp_years= str_replace_all(code_exp_years, "I have never written code", "None"))
```

A lot of columns end with a technology or choice so we rename the columns based the row selection choice. Also cleaning the column names to standard format. 

```{r eval = F}
rename.columns<- function(df){
items<- rep(NA, dim(df)[2])
for (i in 1:ncol(df)){
  items[i]<- str_extract(df[1,i], "[-].*")
  items[i]<- str_remove(items[i], "- Selected Choice -  |- Selected Choice -")
  items[i]<- str_remove(items[i], "\\(.*\\)")
  items[i]<- str_replace(items[i], "[:punct:]\\s", "")
  items[i]<- str_remove_all(items[i], "\\s$|^\\s")
  items[i]<- str_replace_all(items[i], "\\s", "_")
  items[i]<- str_remove(items[i], "_$")
  names(df)[names(df) == names(df[i])]<-items[i]
}
df<- df %>% slice(2:dim(df)[1])
return(df)
}
```

Rip throught the subsets for cleaning.

```{r eval = F}
# Primary Tool for DA at school/work
df_prime_tool<- df %>% 
  select(Q14 )%>%
  slice(2:dim(df)[1]) %>%
  rename(primary_analysis_tool = Q14)
# ML year EXP
df_ml_xp<- df %>% select(Q23) %>%
  slice(2:dim(df)[1]) %>%
  rename(ml_exp_years = Q23) %>%
  mutate(ml_exp_years = str_remove_all(ml_exp_years, "years"))
# language to learn first?
df_lang_rec<- df%>% 
  select(Q19) %>%
  slice(1:dim(df)[1]) %>%
  rename(rec_lang = Q19)
#media sources for DS
df_media<- df %>% select(Q12_Part_1:Q12_Part_12) %>%
  slice(2:dim(df)[1]) %>%
  rename(twiter = Q12_Part_1,
         hacker_news = Q12_Part_2,
         reddit = Q12_Part_3,
         kaggle = Q12_Part_4,
         course_forums = Q12_Part_5,
         you_tube = Q12_Part_6,
         podcast = Q12_Part_7,
         blogs = Q12_Part_8,
         journals = Q12_Part_9,
         slack_communities = Q12_Part_10,
         none = Q12_Part_11,
         other = Q12_Part_12)
# DS course work including University
df_online_ed<- df %>% 
  select(Q13_Part_1:Q13_Part_12) 
df_online_ed<- rename.columns(df_online_ed) 
# IDE
df_ide <- df %>% 
  select(Q16_Part_1:Q16_Part_12) 
df_ide<-rename.columns(df_ide)
# language
df_lang<- df %>%
  select(Q18_Part_1:Q18_Part_12) 
df_lang<- rename.columns(df_lang)
# data viz
df_viz <- df %>% 
  select(Q20_Part_1:Q20_Part_12) 
df_viz<- rename.columns(df_viz) 
# ML algo used regualrly
df_ml_algo<- df %>%select(Q24_Part_1:Q24_Part_12) 
df_ml_algo<-rename.columns(df_ml_algo)
# ML Tools (e.g. AutoML)
df_ml_tool<- df %>% select(Q25_Part_1:Q25_Part_8) 
df_ml_tool<- rename.columns(df_ml_tool) 
# CV tools
df_cv<- df %>% select(Q26_Part_1:Q26_Part_7)
df_cv<-rename.columns(df_cv)
# NLP tools
df_nlp<- df %>% select(Q27_Part_1:Q27_Part_6)
df_nlp<- rename.columns(df_nlp)
# Cloud platform
df_cloud<- df %>% select(Q29_Part_1:Q29_Part_12)
df_cloud<- rename.columns(df_cloud)
# cloud products
df_cloud_prod<- df %>% select(Q30_Part_1:Q30_Part_12)
df_cloud_prod<- rename.columns(df_cloud_prod)
# big data platform
df_big_data<- df %>%select(Q31_Part_1:Q31_Part_12)
df_big_data<- rename.columns(df_big_data)
# cloud ML (e.g. Sagemaker)
df_cloud_ml<- df %>% select(Q32_Part_1:Q32_Part_12)
df_cloud_ml<- rename.columns(df_cloud_ml)
# relational DB
df_db<- df %>% select(Q34_Part_1:Q34_Part_12)
df_db<- rename.columns(df_db)
```


### Prepare the Data for Importing to the Cloud

After some initial data wrangling with R, we want to normalize and map the data correctly for importing to our Azure SQL Server.

1. Remove any special characters from the initial wrangling (bear in mind yes **gsub** was an alternative, but for brevity and readability used **str_replace**).
2. Create reference tables from the factors.
3. Finally Update the transactional entity with foreign key references after each reference data frame was created.


```{r db-data-prep, eval = F}
df_basic %<>% mutate(education = str_replace(education,"â€™",""))

age_ref <- distinct(df_basic, age) %>% mutate(ID = 1:nrow(.)) %>% 
  rename(Description = age)

df_basic %<>% inner_join(age_ref, by = c("age" = "Description")) %>% 
  select(-c(age)) %>% rename(Age = ID) 

country_ref <- distinct(df_basic, country) %>% mutate(ID = 1:nrow(.)) %>% 
  rename(Description = country)

df_basic %<>% inner_join(country_ref, by = c("country" = "Description")) %>% 
  select(-c(country)) %>% rename(Country = ID) 

education_ref <- distinct(df_basic, education) %>% filter(!is.na(.)) %>%
  mutate(ID = 1:nrow(.))  %>%  rename(Description = education)

df_basic %<>% left_join(education_ref, by = c("education" = "Description")) %>% 
  select(-c(education)) %>% rename(Education = ID)

company_size_ref <- distinct(df_basic, company_size) %>% filter(!is.na(.)) %>% 
  mutate(ID = 1:nrow(.)) %>% rename(Description = company_size)

df_basic %<>% left_join(company_size_ref, by = c("company_size" = "Description")) %>% 
  select(-c(company_size)) %>% rename(CompanySize = ID)

ds_team_size_ref <- distinct(df_basic, ds_team_size) %>% filter(!is.na(.)) %>% 
  mutate(ID = 1:nrow(.)) %>% rename(Description = ds_team_size)

df_basic %<>% left_join(ds_team_size_ref, by = c("ds_team_size" = "Description")) %>% 
  select(-c(ds_team_size)) %>% rename(DataScienceTeamSize = ID)

company_use_ml_ref <- distinct(df_basic, company_use_ml) %>% filter(!is.na(.)) %>% 
  mutate(ID = 1:nrow(.)) %>% rename(Description = company_use_ml)

df_basic %<>% left_join(company_use_ml_ref, by = c("company_use_ml" = "Description")) %>% 
  select(-c(company_use_ml)) %>% rename(CompanyUseMachLearn = ID)

compensation_USD_ref <- distinct(df_basic, compensation_USD) %>% filter(!is.na(.)) %>% 
  mutate(ID = 1:nrow(.)) %>% rename(Description = compensation_USD)

df_basic %<>% left_join(compensation_USD_ref, by = c("compensation_USD" = "Description")) %>% 
  select(-c(compensation_USD)) %>% rename(Salary = ID)

gender_ref <- distinct(df_basic, gender) %>% filter(!is.na(.)) %>% 
  mutate(ID = 1:nrow(.)) %>% rename(Description = gender)

df_basic %<>% left_join(gender_ref, by = c("gender" = "Description")) %>% 
  select(-c(gender)) %>% rename(Gender = ID)

code_exp_ref <- distinct(df_basic, code_exp_years) %>% filter(!is.na(.)) %>% 
  mutate(ID = 1:nrow(.)) %>% rename(Description = code_exp_years)

df_basic %<>% left_join(code_exp_ref, by = c("code_exp_years" = "Description")) %>% 
  select(-c(code_exp_years)) %>% rename(YearsOfCodeExperience = ID)

cloud_work_ref <- distinct(df_basic, spend_ml_cloud_work_USD) %>% filter(!is.na(.)) %>% 
  mutate(ID = 1:nrow(.)) %>% rename(Description = spend_ml_cloud_work_USD)

df_basic %<>% left_join(cloud_work_ref, by = c("spend_ml_cloud_work_USD" = "Description")) %>% 
  select(-c(spend_ml_cloud_work_USD)) %>% rename(MachLearnCloudWorkExpense = ID)
```

### Write the Data into the Cloud

We used schemas in our backend because in real world projects that have more than one team using a database, schemas are used to partition authorization and identify what objects belong to what resources.

1. For schemas we must use the relatively new function **Id** from **DBI** Package
2. Next we use **dbWriteTable** function, and send in the dataframes we created earlier.
3. To ensure the correct schema is used, in the name argument we send the **Id** objects we created instead of a simple string that would normally just identify a root level table.
4. Finally insert the participant data (luckily **DBI** recently upgraded to handle batch processing)

```{r data-inserts, eval = F}
kaggle_ref <- Id(schema = "Kaggle", table = "Age")
dbWriteTable(conn, name = kaggle_ref, value = age_ref, append = T)

kaggle_ref <- Id(schema = "Kaggle", table = "Country")
dbWriteTable(conn, name = kaggle_ref, value = country_ref, append = T)

kaggle_ref <- Id(schema = "Kaggle", table = "Education")
dbWriteTable(conn, name = kaggle_ref, value = education_ref, append = T)

kaggle_ref <- Id(schema = "Kaggle", table = "DataScienceTeamSize")
dbWriteTable(conn, name = kaggle_ref, value = ds_team_size_ref, append = T)

kaggle_ref <- Id(schema = "Kaggle", table = "CompanyUseMachLearn")
dbWriteTable(conn, name = kaggle_ref, value = company_use_ml_ref, append = T)

kaggle_ref <- Id(schema = "Kaggle", table = "Salary")
dbWriteTable(conn, name = kaggle_ref, value = compensation_USD_ref, append = T)

kaggle_ref <- Id(schema = "Kaggle", table = "Gender")
dbWriteTable(conn, name = kaggle_ref, value = gender_ref, append = T)

kaggle_ref <- Id(schema = "Kaggle", table = "YearsOfCodeExperience")
dbWriteTable(conn, name = kaggle_ref, value = code_exp_ref, append = T)

kaggle_ref <- Id(schema = "Kaggle", table = "MachLearnCloudWorkExpense")
dbWriteTable(conn, name = kaggle_ref, value = cloud_work_ref, append = T)

kaggle_ref <- Id(schema = "Kaggle", table = "Participant")
dbWriteTable(conn, name = kaggle_ref, value = df_basic, append = T)
```

Finally our last task is to create artifact tables of genre specific surveys that were extracted from the wide table. Using these specific files may help us draw analysis from different features to aid in discovery of valuable information.

```{r artifact-writes, eval=F}
file <- "./Data/df_complete.csv"

df_complete <- read.csv(file = file, sep = ",", stringsAsFactors = F, na.strings = T) %>% 
              mutate_if(is.character, list(~na_if(.,""))) 

dbWriteTable(conn, name = "df_complete",value = df_complete)
dbWriteTable(conn, name = "df_prime_tool",value = df_prime_tool)
dbWriteTable(conn, name = "df_ml_xp",value = df_ml_xp)
dbWriteTable(conn, name = "df_lang_rec",value = df_lang_rec)
dbWriteTable(conn, name = "df_media",value = df_media)
dbWriteTable(conn, name = "df_online_ed",value = df_online_ed)
dbWriteTable(conn, name = "df_ide",value = df_ide)
dbWriteTable(conn, name = "df_lang",value = df_lang)
dbWriteTable(conn, name = "df_viz",value = df_viz)
dbWriteTable(conn, name = "df_ml_algo",value = df_ml_algo)
dbWriteTable(conn, name = "df_ml_tool",value = df_ml_tool)
dbWriteTable(conn, name = "df_cv",value = df_cv)
dbWriteTable(conn, name = "df_nlp",value = df_nlp)
dbWriteTable(conn, name = "df_cloud",value = df_cloud)
dbWriteTable(conn, name = "df_cloud_prod",value = df_cloud_prod)
dbWriteTable(conn, name = "df_big_data",value = df_big_data)
dbWriteTable(conn, name = "df_cloud_ml",value = df_cloud_ml)
dbWriteTable(conn, name = "df_db",value = df_db)
```

### Azure SQL Server Database {.tabset}

#### Expose One Connection for Everyone to Use

```{r eval = F}
conn <- dbConnect(odbc(),
                  Driver = "SQL Server",
                  Server = "spsangelclaudio.database.windows.net",
                  Database = "DataScienceValue",
                  UID = "***************",
                  PWD = "***************")
```

```{r CREATE-DBCONNECTION, include = F}
conn <- dbConnect(odbc(),
                  Driver = "SQL Server",
                  Server = "spsangelclaudio.database.windows.net",
                  Database = "DataScienceValue",
                  UID = "spsadmin",
                  PWD = "bKuNsh6bphei23c")
```

#### Entity Relationship Diagram
![](https://raw.githubusercontent.com/AngelClaudio/DataSources/master/ERD_DataScienceValue.JPG) 

#### Artifact Relations Diagram
![](https://raw.githubusercontent.com/AngelClaudio/DataSources/master/ArtifactRelations.JPG) 

#### Example View
![](https://raw.githubusercontent.com/AngelClaudio/DataSources/master/Example_View.JPG) 

## Kaggle Analysis Part I {.tabset}

### Limitations

The data from kaggle has several limitations. Firstly, the data was survey based, which means it suffers
from several biases. It only samples a subspace of data scientists, as the respondents were found “primarily
through Kaggle channels”. It also suffers from self-selection bias as not everyone who was sent the survey
responded. In addition: not every surveyee was shown every question, and if a person answered “Other” to
a question it is impossible to match their answer with the rest of their answers due to the format of the data.
One of the consequences of said limitations is that the data has a large amount of NA values, likely due to
respondents either choosing not to answer or not being shown certain questions. Another problem is the lack
of precision: since each question is multiple choice, continuous variables like salary or years of experience
lack precision as they have been chunked into discrete bins.

### Data Cleaning

**1. Get View Object for Use**

```{r}
#kaggle_data <- tbl(conn, "uvwParticipantData")
kaggle_data <- dbGetQuery(conn, 'Select * from uvwParticipantData')
```

**2. Check for Outliers**

There are no possible numeric outliers as users did not input numbers by hand but instead chose a bin
provided by the survey writers.

**3. Clean salary data to make it numeric, allowing it to be easily plotted and analyzed.**

```{r}
##This function takes numeric bins and returns the mean of the
##bin, allowing us to more easily order/graph them.
clean_numeric_bins <- function(series){
series <- str_remove_all(series, '[,><+/s]') %>% str_split('-')
series <- sapply(series, as.integer) %>% sapply(mean) %>% sapply(round)
series
}
df_basic <- kaggle_data
df_basic$Salary <- clean_numeric_bins(df_basic$Salary)
```


### Do age and gender affect compensation? {.tabset}

#### Univariate Plots

First we examine the distributions of age, gender, and compensation on their own.

```{r}
df_basic %>% ggplot(aes(x = Age, fill = Gender)) +
  geom_histogram(alpha = 0.5, stat = 'count', position = 'identity')

mean(df_basic$Gender == 'Male')
```

The first thing we notice is that the sample is extremely skewed, made up of almost predominantly males. In fact, almost 84% of those surveyed are male. The most common age group is 25-29, and it appears the median age is likely somewhere around 30.

```{r}
df_basic %>% ggplot(aes(factor(Salary))) +
  geom_bar(stat = 'count') +
  theme(axis.text.x = element_text(angle = 90))
```

We see that the mode salary is extremely low, 500$. This is unexpected, and likely related to people filling out the survey who are not yet working as data scientists still labeling themself as a data scientist.

#### Bivariate Plots

Now we will examine how age and compensation and gender and compensation vary together.

I plot these using mosaic plots, where the area of a rectangle represents the proportion of people falling into both groups.

```{r}
df_basic %>%
  ggplot() +
  geom_mosaic(aes(x = product(Age, Salary), fill = Age),
              na.rm = TRUE) + xlab('Salary') + ylab('Age') +
  ggtitle('Salary by Age Group') +
  theme(axis.text = element_text(size = 6), axis.text.x = element_text(angle = 90))

df_basic %>%
  ggplot() +
  geom_mosaic(aes(x = product(Gender, Salary),
                  fill = Gender), na.rm = TRUE) +
  xlab('Salary') + ylab('Gender') +
  ggtitle('Salary by Gender') +
  theme(axis.text = element_text(size = 6), axis.text.x = element_text(angle = 90))

df_basic %>% ggplot(aes(x = Gender, y = Salary,
                        fill = Gender)) + geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90))

df_basic %>% ggplot(aes(x = Age, y = Salary, fill = Age)) + geom_boxplot()
```

There is a clear distinction between male and female salaries, and a clear upward trend between age and salary.

### Education, Country, and Experience vs. Salary {.tabset}

#### Univariate Plots

First we get a feeling for each variable on its own via a bar plot. I am going to apply the same approach to years of experience to make it more useable in a plot.

```{r, warning = FALSE}
df_basic$Education <- df_basic$Education %>%
  str_replace_all('Some collegeuniversity study without earning a bachelors', 'Some college') %>%
  str_replace_all('No formal education past high school', 'High school')

df_basic %>% ggplot(aes(x = forcats::fct_infreq(Education))) +
  geom_bar(stat = 'count') +
  theme(axis.text.x = element_text(angle = 90)) +
  xlab('Education')

df_basic %>% ggplot(aes(x = forcats::fct_infreq(Country))) +
  geom_bar(stat = 'count') +
  theme(axis.text.x = element_text(angle = 90, size = 7)) +
  xlab('Country')

df_basic$codeExperience<- clean_numeric_bins(df_basic$YearsOfCodeExperience)
df_basic %>% ggplot(aes(x = factor(codeExperience))) +
  geom_bar(stat = 'count') +
  theme(axis.text.x = element_text(angle = 90)) +
  xlab('Years of Coding Experience')
```

We see that the most common level of education is a master's degree, the most common country is India followed by the United States, and the most common level of coding experience is 2 years.


#### Bivariate Plots

```{r init-bi-var-plot, warning = FALSE}
df_basic %>%
  ggplot() +
  geom_mosaic(aes(x = product(Education, Salary),
                  fill = Education), na.rm = TRUE) +
  xlab('Salary') + ylab('Education') +
  ggtitle('Salary by Education') +
  theme(axis.text = element_text(size = 6), axis.text.x = element_text(angle = 90))

india_us <- df_basic %>% filter(Country == 'United States of America'|Country == 'India')

india_us %>%
  ggplot() +
  geom_mosaic(aes(x = product(Country, Salary),
                  fill = Country), na.rm = TRUE) +
  xlab('Salary') + ylab('Country') +
  ggtitle('Salary by Country') +
  theme(axis.text = element_text(size = 6), axis.text.x = element_text(angle = 90))

df_basic %>%
  ggplot() +
  geom_mosaic(aes(x = product(factor(codeExperience), Salary),
                  fill = factor(codeExperience)),
              na.rm = TRUE) + xlab('Salary') +
  ylab('Coding Experience') +
  ggtitle('Salary by Coding Experience') +
  theme(axis.text = element_text(size = 6), axis.text.x = element_text(angle = 90))
```

We find mostly what we would expect. Compensation goes up with education, is HIGHLY affected by country of origin, as US compensation greatly outpaces India's, and goes up with years of coding experience.

```{r codeexp-plot, warning = FALSE}
df_basic %>% ggplot(aes(Education, Salary,
                        fill = Education)) + geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90), legend.position = 'None')

india_us %>% ggplot(aes(Country, Salary, fill = Country)) +
  geom_boxplot() + theme(legend.position = 'None')

df_basic %>% ggplot(aes(factor(codeExperience), Salary,
                        fill = factor(codeExperience))) +
  geom_boxplot() + xlab('Years of Coding Experience') +
  theme(legend.position = 'None')
```

### Beyond the basic dataframe

We have lots of information broken down into many sub dataframes. I will attempt to access this information via a function rather than doing each dataframe individually.

```{r}
rm(india_us)

df_list <- list('df_ide', 'df_lang', 'df_viz', 'df_ml_algo', 'df_ml_tool', 'df_cv',
      'df_nlp', 'df_cloud', 'df_cloud_prod', 'df_big_data', 'df_cloud_ml', 'df_db')
```

```{r salary-plot, warning = FALSE}
### Function to summarize the smaller dataframes into medians and counts
summarize_df <- function(df_name){
  query <- paste('Select * from ', df_name, collapse = '')
  func_df <- dbGetQuery(conn, query)
  names(func_df) <- names(func_df) %>%
    str_remove_all("[/,-]") %>% str_replace_all('\\+', 'plus')
  titles <- c()
  med <- c()
  counts <- c()
  mega_df <- cbind(df_basic, func_df)

  colNames <- names(func_df)

  ## iterate through the columns of the dataframe
  ## each column is one skill
  for (j in colNames){
    ## Calculate the median and count for each column
    meds <- mega_df %>% group_by_(j) %>%
      summarize(med = median(Salary, na.rm = T), count = n())
    titles <- append(titles, meds[[1]])
    med <- append(med, meds[[2]])
    counts <- append(counts, meds[[3]])
  }
  ##Create a dataframe of all skill names, medians, counts
  return(data.frame('skill' = titles, 'medians' = med, 'counts' = counts))
}
```

```{r primetool-plot, warning = FALSE, eval=T}

##Plot side dataframes that are in slightly different format

df_prime_tool <- dbGetQuery(conn, 'Select * from df_prime_tool')
cbind(df_basic, df_prime_tool) %>%
  ggplot(aes(x = str_wrap(primary_analysis_tool, 12),
             y = Salary, fill = primary_analysis_tool)) +
  geom_boxplot() + theme(axis.text.x = element_text(size = 6),
                      legend.text = element_text(size = 6)) +
  xlab('Primary Analysis Tool') +
  theme(legend.position = 'none')

df_ml_xp <- dbGetQuery(conn, 'Select * from df_ml_xp')
df_ml_xp$ml_exp_years <- clean_numeric_bins(df_ml_xp$ml_exp_years)
cbind(df_basic, df_ml_xp) %>%
  ggplot(aes(x = factor(ml_exp_years), y = Salary,
             fill = factor(ml_exp_years))) +
  geom_boxplot() + xlab('Years of ML Experience') +
  theme(legend.position = 'None')

##Plot other side dataframes using function created.
for (i in df_list){
  winnerdf <- summarize_df(i)
  winnerdf <- winnerdf %>%
    filter(!(skill %in% c('None', 'Other'))) %>%
    filter(!(is.na(skill)))

  plots <- winnerdf %>% ggplot(aes(counts, medians,
                        color = str_wrap(skill, 15))) +
    geom_point() + theme(legend.text = element_text(size = 5),
                         legend.title = element_blank()) +
    xlab('Popularity (counts)') +ylab('Median Salary')
  print(plots)
}

```

## Kaggle Analysis Part II {.tabset}

### Limitations

In addition to limitations discussed by Sam regarding the Kaggle data, the KMeans algorithm is not a perfect or definitive method of determining groups of skill sets. KMeans is unsupervised and only separates into pre-defined clusters. One way around this issue is to try many values of K and find the "elbow" point of the graph of the distance between the centers of the clusters. This is still imperfect in that the user must choose the point and it is a bit ambiguous. I settled on four clusters using this method and investigating K values of 3,4, and 5. There is no promise that data scientists only fall into four natural groups, so that will always be a limitation of KMeans clustering. 

### Kaggle Data Science Survey - K Means Cluster Analysis {.tabset}

Our approach for this data set is to quantify the top data science skills using salary of employed data scientists. This survey published by Kaggle gathers information about data scientists, the technologies and languages they use as well as salary and demographics. 

We will try to find the optimum number of clusters that represent the types of people who are in the data science field and discuss differences and commonalities in skills and experience as lens to the most valuable data science skills. 

#### Business Use Case

The type of data science skills that are most valuable are going to be the skills that employers want when building and improving a data science team. To that end, we will use the traits of the clusters to understand what types of skills and experience levels will build the most well-rounded team.  

#### Pull In Data From The Cloud

Everything is loaded on to a SQL server in the cloud

See `kmeans_kaggle.rmd` for upload code. 

We need to bring in the full dataset and convert it into categorical dummy variables. 

```{r}
conn <-                 # Create a connection to your SQL server
    dbConnect(
      MySQL(),
      username = "root",
      password = params$pwd,
      # For rmd on github use: 
      # rstudioapi::askForPassword("Database password")
      # For knitting use:
      # params$pwd
      host = '34.68.193.229'
    )
dbSendQuery(conn, "USE Kaggle_DS_survey;")  

```

```{r}
df_complete <- as.data.frame(tbl(conn, "df_complete"))
df_complete_cat<-df_complete %>% select(-row_names)
df_complete_cat<-dummy_cols(df_complete_cat, remove_first_dummy = T)

df_complete_cat<-df_complete_cat[ ,(ncol(df_complete)+1):ncol(df_complete_cat)]
df_complete_cat[is.na(df_complete_cat)]<--1
```


#### K Means Clustering

Running the algorthim 100 times and averaging the results. 


```{r fig.width=8, fig.height=4,  warning=FALSE}
run_kmean <- TRUE
if (run_kmean == TRUE){
set.seed(9450)
k_range<-2:30
trials <-100                   # Run K Means 100 times to average 
avg_tot_wss <-integer(length(k_range)) 
for(v in k_range){ 
 vec_tot_wss <-integer(trials) 
 for(i in 1:trials){
  k_tmp <-kmeans(df_complete_cat,centers=v)          # Run kmeans
  vec_tot_wss[i] <-k_tmp$tot.withinss
 }
 avg_tot_wss[v-1] <-mean(vec_tot_wss)    # Average total withinss
}
}
plot(k_range,avg_tot_wss,type="b", main="Total Within SS by Various K",
 ylab="Average Total Within Sum of Squares",
 xlab="Value of K")
```

Four clusters seems to be a good fit. I explored a few other options, but four seems like meaningful number of groups for this data. 

Below we assign the cluster number to the complete dataframe of all questions. 

```{r}
# Assign the cluster values
five_k<-kmeans(df_complete_cat,centers=4)
df_complete$cluster<- as.factor(five_k$cluster)
```

### Clusters Analysis for Data Science Skills {.tabset}

We have four groups and they demonstrate how this field is varied in skills, technologies, and people claiming the title. 

#### Age and Compensation

Below is a plot of the spread of the compensation by age range for each cluster. 

Clusters 3 and 4 appear to be similar for these categories where as cluster one and two appear distinict. Cluster two appears to be newcomers to data science with little experience and salary. 

```{r fig.width=8, fig.height=6}
df_complete %>% ggplot(aes(x = age, y = compensation_USD, fill = age)) +
  geom_boxplot(na.rm = T) + 
  facet_wrap(~cluster) +
  labs(x = "Age", y = "Salary", title= "Distribution of Salary by Age") +
  theme(axis.text.x = element_text(angle = 45, size=14))  
```

#### Education and Compensation

Below is compensation vs. education level for each cluster. In cluster one, the spread is large for people without a degree. Cluster one also had very highly paid people in the 55+ age ranges, so our data science managers might be in this group. Clusters three and four appear similar in this category, though there are some people in cluster three without a degree. 


```{r fig.width=8, fig.height=6}
df_complete$education<-str_replace_all(
    df_complete$education, 
    "Some collegeuniversity study without earning a bachelors", 
    "No Degree") 
df_complete %>%
  ggplot(aes(x = education, 
             y = compensation_USD, 
             fill = education)) +
  geom_boxplot(na.rm = T) +
  facet_wrap(~cluster) +
  labs(x = "Education",
       y = "Salary", 
       title= "Distribution of Salary by Education Level") +
  theme(axis.text.x = element_text(angle = 45, size=14))
```

#### Code Experience and Compensation

This is a nice separator for our field. Cluster one are people with a lot of experience and are paid well for it. Cluster two are, in fact, newcomers to tech. Clusters three and four are similar in terms of compensation and code experience except for some changes in variance. 

```{r fig.width=8, fig.height=6}
df_complete %>% ggplot(aes(x = code_exp_years, y = compensation_USD, fill = code_exp_years)) + 
  geom_boxplot(na.rm = T) + 
  facet_wrap(~cluster) +
  labs(x = "Years Experience", y = "Salary", title= "Distribution of Salary by Code Experience") +
  theme(axis.text.x = element_text(angle = 45, size=14))
```


### Categorical Results {.tabset}

Next we will look at the frequency with which each cluster uses various data science technologies. This work compliments our web scrape data by quantifying the relative use of established and emerging technologies. 

Below is a plot function for the categories. 

```{r}
# Categorical names changes and cleaning
rename.func<-function(x){
    x<-str_remove(x, "[:punct:]\\d")
    x<-str_remove(x, "\\(.*\\)")
    x<-str_remove(x, "(.*_)")
# Shorten long product names
    x<-str_replace_all(x, 
                      "Google Cloud", "GCP")
    x<-str_replace_all(x, 
                       "Amazon", "AWS")
    x<-str_replace_all(x,
                       "Azure Machine Learning", "Azure ML")
    return(x)
    }

cluster.category.count.plot<-function(df_basic, df_cat, clusters,shorten){
  
  # This function takes a categorical dataframe and joins it to 
  # the main demographic dataframe and collects the frequency of
  # each selection. 
  
  df<-df_cat %>% select(-row_names) # Remove unneeded columns
  df<-subset(df, select = -c(None, Other))
  df_names<-names(df)
  df<-df_basic %>%
    select(-row_names) %>%          # Join and change data type
    inner_join(df, by = "id") %>%
    filter(country == "United States of America") %>%
    mutate(cluster = as.factor(clusters)) %>%
    select(cluster,df_names)
  df<-dummy_cols(.data = df,        # Convert to "dummy" variable
                 select_columns = df_names,
                 remove_first_dummy = T, 
                 remove_selected_columns = T)
  df<-df[,1:(length(df_names))]
  df_names<-names(df)

  df<-df %>%                        # Tidy and prep for plot
   select(cluster, df_names) %>%
   mutate_if(is.factor, as.integer) %>%
   pivot_longer(-cluster, 
                names_to = "category_type", 
                values_to = "count") %>%
   group_by(cluster, category_type) %>%
   summarise(count = sum(count))
  
  df<-as.data.frame(apply(df, 2,
                          FUN=rename.func)) 
  
  df %>%
    ggplot(aes(x = category_type,   # Plot counts
               y = count,
               fill = category_type)) +
    geom_col(na.rm = T) +
    facet_wrap(~cluster) +
    theme(axis.text.x = element_text(angle = 45, 
                                     size = 14))
}
```


#### Primary IDE

Here we see that, save for a few IDE preferences, most clusters of people use in some capacity a variety of IDEs. Or have used them at some point. Cluster three and four heavily use many of these IDEs.

```{r fig.width=8, fig.height=6, warning=F}
df_basic<-as.data.frame(tbl(conn, "df_basic"))
df_ide<-as.data.frame(tbl(conn, "df_ide"))
cluster.category.count.plot(df_basic, df_ide, five_k$cluster) +
    labs(x = "IDE", y = "Count", title= "Frequency of IDE Use")
```

#### Primary Language

Here we see the big three; Python, R, SQL are very popular across all the clusters with Bash in fourth place. Our older, more experienced cluster, cluster one, has experience in wide variety of languages. Interestingly, clusters three and four separate on the their view of R. 


```{r fig.width=8, fig.height=6}
df_lang<-as.data.frame(tbl(conn, "df_lang"))
cluster.category.count.plot(df_basic, df_lang, five_k$cluster) + 
    labs(x = "Languages", y = "Count", title= "Frequency of Language Use")

```

#### Big Data Cloud Services

Here cluster one and four are both types of data scientists that use big data service at work and cluster three is not. AWS Redshift, Databricks, and Google BigQuery seem to be the favorites - they are all structured data servers, which makes sense given the primacy of SQL. 

```{r fig.width=8, fig.height=8}
df_big_data<-as.data.frame(tbl(conn, "df_big_data"))
cluster.category.count.plot(df_basic, df_big_data, five_k$cluster) +
    labs(x = "Big Data Product", y = "Count", title= "Frequency of Big Data Use")
```

#### Cloud Machine Learning

Again, clusters one and four use cloud based ML services. Interestingly, AWS Sagemaker is the clear favorite for those clusters. A good technology to learn. 

```{r fig.width=8, fig.height=8}
df_cloud_ml<-as.data.frame(tbl(conn, "df_cloud_ml"))
cluster.category.count.plot(df_basic, df_cloud_ml, five_k$cluster)+
    labs(x = "Cloud Product", y = "Count", title= "Frequency of Cloud Machine Learning Use")
```


#### Relational Database

Here we see that having solid skills in the traditional SQL database technologies is extremely helpful. Cluster one, who have been in the business for a while and use newer technologies is adapting cloud-based technology for relational databases. This is also true for cluster four

```{r fig.width=8, fig.height=8}
df_db<-as.data.frame(tbl(conn, "df_db"))
cluster.category.count.plot(df_basic, df_db, five_k$cluster) +
    labs(x = "Database Type", y = "Count", title= "Frequency of Relational Database Use")

```

## KD Nuggets Analysis Part I {.tabset}

**Packages Used**

The packages I used during my data gathering and analysis were `tidyverse`, `rvest`, `stringi`, `RCurl`, `xml2`, `kableExtra`, and `Stack`.

### Core DS Skills {.tabset}

In a study published on [KD Nuggets](https://www.kdnuggets.com/2019/09/core-hot-data-science-skills.html), a group of 1500 Data Scientists were asked the following questions:

1. Which skills / knowledge areas do you currently have (at the level you can use in work or research)?

2. Which skills do you want to add or improve?

The data from this poll was seperated into 3 seperate tables. The steps outlined below illustrate the process of scraping the web page for specific tables, stacking the tables and tidying the data so that it can be visualized.

```{r, warning = FALSE}
#KD Nuggets website with tables
urlTen <- "https://www.kdnuggets.com/2019/09/core-hot-data-science-skills.html"

#Parse the html file
urlTen <- read_html(urlTen)

#Reads the tables in the html page and places them in a list
KdTables <- html_nodes(urlTen, "table") %>%
  html_table(fill = TRUE)

#Select each table and add a column with category name (for plotting purposes)
kdTableOne <- KdTables[[1]] %>% mutate(Type = as.factor("Established Skills"))
kdTableTwo <- KdTables[[2]] %>% mutate(Type = as.factor("New Skills"))
kdTableThree <- KdTables[[3]] %>% mutate(Type = as.factor("Other"))

#Stack the Data Frames into one
combinedKdTable <- Stack(kdTableOne, kdTableTwo)
combinedKdTable <- Stack(combinedKdTable, kdTableThree)

#Rename Columns
combinedKdTable <- rename(combinedKdTable, "want" = "%Want", "have" = "%Have","wantHaveRatio" ="%Want/%Have") %>% gather("Category", "Percent", "have":"wantHaveRatio")

#Take away % and cast to numeric for plotting
combinedKdTable$Percent <- as.numeric(unlist(str_remove_all(combinedKdTable$Percent, "\\%")))

#Put back into wide format for plotting
combinedKdTable<- spread(combinedKdTable, Category, Percent) %>% select(Skill, want,have, Type)
```

As a result the following table and plot was generated.

```{r, warning =FALSE}
#Display Table
kable(combinedKdTable[1:3]) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

#plotting have vs want
ggplot(combinedKdTable, aes(x=have, y=want, color = Type)) +
  scale_color_brewer(palette="Dark2") +
  labs(y = "Percent Want", x ="Percent Have", title = "Skills Data Scientists Want vs Have (Clustering)") +
  theme(plot.title = element_text(hjust = 0.5), legend.position="bottom", legend.box = "horizontal") +
  geom_label(label=combinedKdTable$Skill, nudge_x = 0.25, nudge_y = 0.25, check_overlap = T)

```

Here we can see clearly that the skills identified in this survey falls into three categories: skills that are well established that have a high percentage of respondants that have them and a low percentage of respondants who want them(green); skills that are in high demand with a high percentage of respondents reporting they want that skill and do not have them(orange); and skills that very few people want and very few people have (purple). These groupings functioned as a basis for the Google Trends Search we will be conducting in the next section.

### Core DS Software {.tabset}

In a study published on [KD Nuggets](https://www.kdnuggets.com/2019/05/poll-top-data-science-machine-learning-platforms.html)
1,800 participants were asked to identify their favorite platforms for Analytics, Data Science and Machine Learning Software in 2019 - 2017. Data was processed and presented in percent of voters.

The data from this poll was seperated into 6 seperate tables. The steps outlined below illustrate the process of scraping the web page for specific tables of  interest and visualizing them using bar plots and a line plot to illustrate trends over time.

#### Top DS Software in 2019 - 2017

To gain some insight into the growth in demand for various software platforms, we decided to look at trends in poll responses from 2017 - 2019 by visualizing the poll data in a line graph.

```{r, warning = FALSE}
#Webpage with Data
urlEleven <- "https://www.kdnuggets.com/2019/05/poll-top-data-science-machine-learning-platforms.html"

#Parse html page
urlEleven <- read_html(urlEleven)

#Reads the tables in the html page
kdTableFour <- html_nodes(urlEleven, "table") %>%
  html_table(fill = TRUE)

#1st Table on the page
topSF <- kdTableFour[[1]]

#Change Column Names
topSF <- rename(topSF, "2019" = "2019% share", "2018" = "2018% share","2017" ="2017% share") %>% gather("Year", "Percent", "2019":"2017")

#set numeric types for line plots
topSF$Percent <- as.numeric(unlist(str_remove_all(topSF$Percent, "\\%")))
topSF$Year <- as.numeric(topSF$Year)

#Display
kable(topSF) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

#Line Plot
ggplot(topSF) +
  geom_line(aes(x=Year, y=Percent, col = Software)) +
  labs(title= "Top Data Science Tools", x = "Year", y = "Percent of People Using Tool") +
  theme(plot.title = element_text(hjust = 0.5))
```

We can see here that Python has been the most popular data science tool all three years. R, although relatively popular, we see that it is steadily declining in popularity. RapidMiner's had the highest spike in popularity between 2017 and 2018 but then began to decline from 2018 - 2019. Tools such as SQL, Tableau and Excel also seem to be decreaing in popularity. Software such as Tensorflow, Anaconda and Keras are increasing in use steadily through this time range.

#### Largest Decrease Use

If we take a closer look at the difference between percent of responses in 2018 and 2019 we can gain some insight into tools that may be losing popularity in the Data Science community (and being replaced).

```{r}
#Select Table 3
declineUse <- kdTableFour[[3]]

#Clean Up the table for analysis
declineUse <- rename(declineUse, "2019" = "2019% share", "2018" = "2018% share","change" ="% change") %>% gather("Year", "Percent", "2019":"change")

#Data Types and Filter
declineUse$Percent <- as.numeric(unlist(str_remove_all(declineUse$Percent, "\\%")))
declineUse <- filter(declineUse, Year == "change")

#Display % change
kable(declineUse) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

#Visualization
ggplot(declineUse, aes(y=Percent, x=reorder(Platform,Percent))) +
  geom_col(fill = "blue") +
  labs(y = "Percent Decline", x ="Data Science Tools", title = "Software With Largest Decrease in Use (2018 - 2019)") +
   theme(plot.title = element_text(hjust = 0.5)) +
  coord_flip()
```

From here we see that several IBM Software are declining in popularity along with other including Microsoft Machine Learning Server.

#### Deep Learning Platform Trends

Along with our goal to analyze tools that are increasing and decreasing popularity we can also take a look at the deep learning platforms identified by data scientists in this poll.

```{r}
# Table 4
dlPlat <- kdTableFour[[4]]

#Rows names and long format
dlPlat <- rename(dlPlat, "2019" = "2019% share", "2018" = "2018% share","change" ="% change") %>% gather("Year", "Percent", "2019":"change")

#Clean data
dlPlat$Percent <- as.numeric(unlist(str_remove_all(dlPlat$Percent, "\\%")))
dlPlat <- filter(dlPlat, dlPlat$Year == "change")

#Display
kable(dlPlat) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

#Visualization
ggplot(dlPlat, aes(y=Percent, x=reorder(Platform,Percent))) +
  geom_col(fill = "lightgreen") +
  labs(y = "Percent Change", x ="Data Science Tools", title = "Deep Learning Platforms Trends (2018 - 2019)") +
   theme(plot.title = element_text(hjust = 0.5)) +
  coord_flip()
```

We can see that Pytorch is gaining popularity at a high rate over 2018 - 2019 whereas tools such as Thenao, Caffe, and Microsoft Cognitive Toolkit are decreasing in popularity.

### Programming Languages

Lastly, we were interested in looking at trends in programming languages through 2018-2019 to gain some insight into languages that may be gaining or losing popularity in data science.

```{r, warning = FALSE}
progLang <- kdTableFour[[6]]

#Clean Up the table for analysis
progLang <- rename(progLang, "2019" = "2019% share", "2018" = "2018% share","change" ="% change") %>% gather("Year", "Percent", "2019":"change")

progLang$Percent <- as.numeric(unlist(str_remove_all(progLang$Percent, "\\%")))
progLang <- filter(progLang, Year == "change") %>% drop_na()
progLang <- filter(progLang, Platform != "Other programming and data languages")

#Display Table
kable(progLang) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

#Visualization of Want and Have
ggplot(progLang, aes(y=Percent, x=reorder(Platform,Percent))) +
  geom_col(fill = "pink") +
  labs(y = "Percent Change", x ="Programming Platforms", title = "Programming Platform Trends (2018 - 2019)") +
   theme(plot.title = element_text(hjust = 0.5)) +
  coord_flip()
```

Interestingly enough, Julia has the highest percent positive change (increasing popularity) while Scala has the highest percent negative change (decreasing popularity). Python seems to be maintaining it's popularity with the lowest percent change in any direction. I think that its interesting that C/C++ saw a positive change. I wonder what thats about.

### Limitations

The data that I ultimately used was data from a poll that KD Nuggets conducted where the sample of people who took the poll were Data Science Professionals. Due to the fact that I did not have access to the raw data I was not able to do any deep statistical analysis or create a model that predicted future skill popularity. In addition, the keywords that were identified as DS Skill, Software or Platform, also presents a limitation in the way we used them. These keywords were taken from this study which was interpreted by those who conducted it therefore, exact keywords may be inconsistent with google trends and searches.

## KD Nuggets Analysis Part II {.tabset}

The KD Nuggets data set provided insight into the evolving trends from 2017-2019 among skills desired for data scientists through the use of single point surveys. Another way to look at ebbs and flow of skill popularity is through the use of Google Trends. Google, the world's most heavily used search engine, tracks the frequency of a search term, and provides normalized time series for comparison. Using Google Trends is not a new concept and there is an abundant [literature](https://www.sciencedirect.com/science/article/pii/S0020025516300846) on how it can serve as a proxy for popularity, and precursor for predictions.

```{r pressure, echo=FALSE, out.width = '50%', fig.align = "center"}
knitr::include_graphics("https://github.com/MsQCompSci/images/blob/master/neil.png?raw=true")
```

### Intro to Google Trends

Google Trends provides a normalized score [0-100] for keywords, relative to its typical number of searches. This should **not** be confused with a sheer magnitude of popularity, defined by absolute search volume. This is important since a keyword can have a decreasing popularity score even though search volumes are increasing--it just indicates that said keyword is a lower \% of searches. For example while a well-known keyword like "Drake" typically has a high absolute number of searches, a lesser known keyword like "Coronavirus" can have a higher score, since the proportion of searches spikes.

Furthermore time/region parameter adds normalization to a specific date range and area.

In addition Google Trends handles low volume searches with a 0 score--this doesn't indicate that there were no searches but that the volumes were not significant. Google Trends will elminate duplicate searches and special characters.

Finally, while we use the term popularity, Google Trends is not a poll on a direction or favorability of a keyword--just search proportion. Hypothetically "serial killers' could have a higher score than "puppies" not because a change in public sentiment but due to higher interest in search.

We can summarize Google Trends data as follows and acknowledge these limitations:

1. Google Trends is not a poll nor does it indicate sentiment but relative proportion of a keyword to other searches.

2. Google Trends will tell us the change in proportion of searches.

With that said we can still use the data to compare popularity in search proportions as a proxy of interest within Data Science.


### gTrendsR Demo

While there is an easy to use Google Trends webpage, there is also a R based package called GtrendsR that provides functionality. GtrendsR accepts a keyword (or keyword vectors), geographical location, cookie parameters, language and time dimension as parameters--the full documentation can be found [here](https://cran.r-project.org/web/packages/gtrendsR/gtrendsR.pdf).

For example--the following call (just the head for brevity) specified the key word "Python programming language" for the US region for a 5 year frame.

```{r}
output <- head(gtrends(keyword = 'Python programming language',
         geo = "US",
        time = "today+5-y"))
names(output)
```

The resultant is a list of dataframes, with categories of interest by time, country, region, demographics, city and related topic. To standardize results we will restrict our search to the "US" and focus on hits, or interest over time.

```{r}
kable(head(output$interest_over_time))
```

The interest_over_time provides a hit score time-series for the search keyword.For example--for "Python Programming Language" over a 5 year time frame.

```{r}
ggplot(data=output$interest_over_time)+geom_line(aes(y=hits,x=date))+ggtitle('Python Programming Language Hits over Time')
```

### Keyword DataFrame Function

We'll be making a direct comparison with the KDnuggets Top Data Science Tools list from the previous section, and compare the evolution in trends over the same time scale, from 2017-2019.

The following code block loops through a list of keywords and populates a dataframe with hits. This general frame work will be used when looking at other keywords.

```{r}
#Using the kaggle Top Software data set and making a timerange to fit
keywords <- sort(unique(topSF$Software))
timerange <-"2017-01-01 2019-01-01"
```

To automate things we made a function called googletrends.

```{r}
googletrends <-function(keywords)
{
timerange <-"2017-01-01 2019-01-01"
#generating a Date vector
date<-select((gtrends(keyword = keywords[1],
                   geo = "US",
                   time = timerange)$interest_over_time),date)
data <-vector(mode = "list", length = length(keywords))
n <-1
for (i in keywords)
{
  print(n)
  print(i)
  data[[n]]<-select((gtrends(keyword = i,
                  geo = "US",
                  time = timerange)$interest_over_time),hits)
  n<-n+1
}
#Making Data Frame
df <- data.frame(data)
colnames(df) <-keywords
df<-cbind(date,df)
}
```

Now running our code
```{r}
kable(df <-googletrends(keywords)) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")
```

### Statistical Summary

First--let's look at the summary statistics and a boxplot to visualize our keywords.

```{r}
kable(summary(df)) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "200px")

par(las=2)
boxplot(df[2:length(df)],main='Google Trends for Top Data Science Tools')
```

Keep in mind this is averaged over the entire time range [2017-2019] but there are a few metrics we can look at.

1. Median/mean which would just tell us overall how popular[once again proportion of searches] a term was searched.

2. The spread via inner quartile range or whisker would be an indication of consistency within the time frame--a large/wide spread (relative) would indicate that a keyword hit had more variability, and changed over the time frame. Once again since this is taken across the entire time-frame, we don't know the overall direction.


Based on the boxplot and statistics we can see:

1. Excel has the highest median [87] and combined with the most narrow spread indicating that it still dominates proportion of Google searches--which makes sense given it's ubiquity and age.

2. Looking at the big Programming languages--Python was the most popular, followed by R and then SQL  [83,75,64 respectively] with the same trend in decreasing spreads--**this matches with KdNuggets about Python's popularity among the big 3**

3. RapidMiner has the largest variability indicating that it rapidly changed popualarity in time--once again **agreeing with KDnuggets** Scikit, Keras and Apache Spark also have the next largest spreads indidcating variability (once again we don't know which direction).

4. Anaconda tied with Python for 2nd highest median--this makes sense due to it's strong correlation with Python (it's the go to package)

### Google Trend Analysis

Now the entire point of Google Trends was to view this evolution of interest over time--so let's plot it!

```{r}
tidydf <- melt(df, 'date')
names(tidydf) <-c('date','Software','Hits')
ggplot(tidydf, aes(x=date, y=Hits, color=Software)) + geom_line()+ xlab('Time')+ylab('Hits')+ggtitle('Google Trends for Top Data Science Tools')
```

No suprise this is a bit noisy due to the high amount of samples.

One common technque when dealing with noisy data is to smooth it out via some sort of moving average--for simplicity we'll employ geo_smooth() which uses a [LOESS](https://en.wikipedia.org/wiki/Local_regression), which is a combination of moving average and regression, to fit the data. This does change the statistics for the data but should keep trends similar.


```{r}
ggplot(tidydf, aes(x=date, y=Hits, color=Software)) + geom_smooth(se=FALSE)+ xlab('Time')+ylab('Hits')+ggtitle('Google Trends for Top Data Science Tools')
```

This is much easier to read--and we can now see how the skills changed in popularity over time!

I'm going to first break this plot down to compare the big languages, Excel, R, SQL and Python.

```{r}
df %>% select(c(names(df)[1],names(df)[4],names(df)[6],names(df)[7],names(df[10])))  %>% melt('date') %>% ggplot(aes(x=date, y=value, color=variable)) + geom_smooth(se=FALSE)+ xlab('Time')+ylab('Hits')+ggtitle('Google Trends for Programming Languages')
```

Interesting enough

1. While Excel started out the most popular [once again relative proportion] language--it waned over time! R and SQL followed a similar trend in decreased popularity, with R and then SQL ending up with less searches.

2. Only Python actually became more popular/searched over time--which aligns with our **KDnugget** data.

3. There was some 2018 bump that happened for all languages--noticable around the beginning of 2018 and then midway through.

From the Google Trends data we can conclude that Python is indeed the more searched/popular language, and has taken away buzz from legacy languages like Excel, SQL and R.

Overall 2019 popularity trend in descending order: **Python, Excel, R and finally SQL**


Now looking at the other variables

```{r}
df %>% select(c(names(df)[1],names(df)[2],names(df)[3],names(df)[5],names(df)[8],names(df)[9],names(df)[11],names(df)[12]))  %>% melt('date') %>% ggplot(aes(x=date, y=value, color=variable)) + geom_smooth(se=FALSE)+ xlab('Time')+ylab('Hits')+ggtitle('Google Trends for Programming Languages')
```

Here we can see that

1. Apache Spark, Tensor flow, Keras actually had the widest variability--with Apache Spark losing popularity overall, while the other two ending up with higher popularity. RapidMinr did not have variability despite the previous data data set showing it--I attribute that to random one off outliers. It actually lost popularity over time

2. It seems that nearly all skills had a "peak" around 2018 and then waned in popularity after.

3.Tableu and Anaconda had similar trends--gaining popularity in early 2017-2018 but then returning back to their hit rate in 2019.

**Overall in terms of popularity at the end of our time frame (start of 2019): Keras, Tabeleau, TensorFlow, Anaconda, Apache, Scikit and Rapidminr**

This is a bit suprising since at first one would think niche or specialized subset skills, like Keras or Scikit would naturally have lower popularity, but this analysis shows that this isn't always the case!


### Comparison to KDnuggets Survey

Now we can do a comparison for KDnuggets versus GTrends to see if there in consensus between the two data sets.

```{r}
ggplot(tidydf, aes(x=date, y=Hits, color=Software)) + geom_smooth(se=FALSE)+ xlab('Time')+ylab('Hits')+ggtitle('Google Trends for Top Data Science Tools')

ggplot(topSF) +
  geom_line(aes(x=Year, y=Percent, col = Software)) +
  labs(title= "Top Data Science Tools", x = "Year", y = "Percent of People Using Tool") +
  theme(plot.title = element_text(hjust = 0.5))

```

### DL Platform Trends

Using a the same methodology above we generate Gtrend data for the keywords within the KD Nuggets Software Dataset.

One caveat: I had to change Other Deep Learning Tools to Deep Learning Tools due to lack of hits.

```{r}
keywords <-sort(unique(dlPlat$Platform))
keywords[6] <- "Deep Learning Tools"
df <-googletrends(keywords)
```

The KDnuggets survey was taken at a single point in time and thus we will apply this to Gtrends by taking two data points. This time we want to compare the change from 2019 and 2018 (given these are our survey data points). Making use of symmetry in our dataframe length--that is 2018 will be the midpoint of the dataframe. We can simply make a new vector with the percent change.

```{r}
nrow(df)
Change <-((df[105,2:length(df)]-df[53,2:length(df)])/df[105,2:length(df)])*100

#Making a function replace NaN

is.nan.data.frame <- function(x)
do.call(cbind, lapply(x, is.nan))

# Dropping NaN with 0s
Change[is.nan(Change)] <- 0
#Converting to datframe and adding Platform for mapping
Change <-data.frame(t(Change))
Change$Platform <-keywords
```

Now plotting in the same manner as the KD Nuggets
```{r}
ggplot(Change,aes(y=X105,x=reorder(Platform, X105)))+geom_col(fill = "lightgreen") +
   labs(y = "Percent Change", x ="Data Science Tools", title = "Deep Learning Platforms Trends (2018 - 2019)") +
    theme(plot.title = element_text(hjust = 0.5)) +
   coord_flip()

#The KdNuggets Plot
ggplot(dlPlat, aes(y=Percent, x=reorder(Platform,Percent))) +
  geom_col(fill = "lightgreen") +
  labs(y = "Percent Change", x ="Data Science Tools", title = "Deep Learning Platforms Trends (2018 - 2019)") +
   theme(plot.title = element_text(hjust = 0.5)) +
  coord_flip()
```

### Programming Platform Trends

Using the same methodology for the Program Platform Trends--and modifying "Unix Shell" and "Other Programming Languages" for Google Trends sanity.

```{r}
keywords <-unique(progLang$Platform)
keywords[5] <- 'Unix Shell'
keywords[7] <- "Other Programming Languages"
df <-googletrends(keywords)
head(df)
ChangePP <-((df[105,2:length(df)]-df[53,2:length(df)])/df[105,2:length(df)])*100
ChangePP[is.nan(ChangePP)] <- 0
ChangePP <-data.frame(t(ChangePP))
ChangePP$Platform <-keywords
```

And plotting again

```{r final-plot}
ggplot(ChangePP,aes(y=X105,x=reorder(Platform, X105)))+
  geom_col(fill = "pink") +
  labs(y = "Percent Change", x ="Programming Platforms", title = "Programming Platform Trends (2018 - 2019)") +
    theme(plot.title = element_text(hjust = 0.5)) +
   coord_flip()

ggplot(progLang, aes(y=Percent, x=reorder(Platform,Percent))) +
  geom_col(fill = "pink") +
  labs(y = "Percent Change", x ="Programming Platforms", title = "Programming Platform Trends (2018 - 2019)") +
   theme(plot.title = element_text(hjust = 0.5)) +
  coord_flip()
```

# Work Cited

1. [Which Data Science Skills are core and which are hot/emerging ones?](https://www.kdnuggets.com/2019/09/core-hot-data-science-skills.html)

2. [Python leads the 11 top Data Science, Machine Learning platforms: Trends and Analysis](https://www.kdnuggets.com/2019/05/poll-top-data-science-machine-learning-platforms.html)

3. [Kaggle Data 2019](https://www.kaggle.com/c/kaggle-survey-2019/data)

4. [rvest Blog](https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/)

5. [DataCamp rvest Tutorial](https://www.datacamp.com/community/tutorials/r-web-scraping-rvest)

6. [Discord (for collaberation)](https://discordapp.com/)

7. [GtrendsR CRAN](https://cran.r-project.org/web/packages/gtrendsR/gtrendsR.pdf)

8. [FAQ about Google Trends](https://support.google.com/trends/answer/4365533?hl=en)

8. [R for Data Science]("R for Data Science" https://r4ds.had.co.nz/)
